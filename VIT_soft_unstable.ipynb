{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIT_soft_unstable.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUXzqUK0eNqz"
      },
      "source": [
        "Этот код содержит реализацию VIT из этой [статьи](https://arxiv.org/abs/2010.11929)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1duVL2MlBFfW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Fgcmt-TrZg",
        "outputId": "30adbe4f-52f4-4507-8ade-9c4a6118eeea"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u8cIRvCBFfX"
      },
      "source": [
        "from einops import rearrange, reduce, repeat\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch.autograd as autograd\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import image\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import Tuple, List, Type, Dict, Any\n",
        "from os.path import join, isfile, isdir\n",
        "from queue import Empty, Queue\n",
        "from threading import Thread\n",
        "\n",
        "# augmentation library\n",
        "from imgaug.augmentables import Keypoint, KeypointsOnImage\n",
        "import imgaug.augmenters as iaa "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd6-62I8geA7",
        "outputId": "62df126e-b9d7-4992-ef4d-6c09f3ba9add"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBco00DHe3ym"
      },
      "source": [
        "##Connect CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDUgFipfe7Rm"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpe2gEKifARC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f4b0d7-7876-4212-8b22-afeeaa8b955c"
      },
      "source": [
        "# Проверим доступность GPU\n",
        "torch.cuda.device_count()\n",
        "if torch.cuda.is_available() :\n",
        "  print(torch.cuda.get_device_properties(DEVICE))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N6VHIKDfbzp"
      },
      "source": [
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR 10 "
      ],
      "metadata": {
        "id": "u2xiUsk-PBX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True, transform=torchvision.transforms.Compose([\n",
        "                                                                torchvision.transforms.ToTensor(),\n",
        "                                                                torchvision.transforms.Resize((224, 224)),\n",
        "                                                      torchvision.transforms.ToPILImage(), \n",
        "                                                     torchvision.transforms.ToTensor()\n",
        "                                                    ]))"
      ],
      "metadata": {
        "id": "vSqm-vcNfcOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edda5c6b-562f-4234-ec32-bad70463fcb7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMYu-eJASznw",
        "outputId": "e494f9f2-dd74-4cb4-8edb-082def382286"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
            "               ToPILImage()\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                       train=False,\n",
        "                                       download=True, \n",
        "                                       transform=torchvision.transforms.Compose([\n",
        "                                                                                 \n",
        "                                                                torchvision.transforms.ToTensor(),\n",
        "                                                                torchvision.transforms.Resize((224, 224)),\n",
        "                                                      torchvision.transforms.ToPILImage(), \n",
        "                                                     torchvision.transforms.ToTensor()\n",
        "                                                    ]))"
      ],
      "metadata": {
        "id": "B2UrvMCTfbwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d777ff2-1ad5-4c7e-ab3f-53033766f5b9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfIdvJbBFfl"
      },
      "source": [
        "## Implement multilayer perceptron (MLP) and additional Conv "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-oZ7AwnBxQr"
      },
      "source": [
        "def init_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sylHqcfYBFfm"
      },
      "source": [
        "class  MLP(nn.Module):\n",
        "  def __init__( self, layers_dims, dropout_rate = 0.1):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layers_dims = layers_dims\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.layers = []\n",
        "    for in_features, out_features in self.layers_dims:\n",
        "        self.layers.append(\n",
        "            nn.Sequential(nn.Linear(in_features, out_features).to(DEVICE),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Dropout(self.dropout_rate))\n",
        "            )\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "        x =layer(x)\n",
        "        \n",
        "       \n",
        "    return x\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MERqUckBFfm"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qJNe8rnBFfn"
      },
      "source": [
        "class Patches(nn.Module):\n",
        "    def __init__(self, patch_size, num_channels = 3):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def __call__(self, images):\n",
        "        #Самая удобная реализация патчинга картинок есть на tensorflow\n",
        "        images = tf.convert_to_tensor(images.detach().cpu().numpy().transpose(0,2,3,1))\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        \n",
        "        patches = tf.reshape(patches, [batch_size,  patches.shape[1]* patches.shape[2], self.patch_size, self.patch_size,self.num_channels])\n",
        "        return torch.Tensor(patches.numpy()).to(DEVICE)\n",
        "\n",
        "    def forward(self, images):\n",
        "        return __call__(images)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysbafQXWBFfo"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "Проецирует линейно на скрытое измерение и добавляет Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGLjLVqbBFfp"
      },
      "source": [
        "\n",
        "class PatchEncoder(nn.Module):\n",
        "    def __init__(self, num_patches,patch_size, projection_dim,num_channels = 3, dropout = 0.1):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = nn.Linear(num_channels*patch_size**2, projection_dim)\n",
        "        self.pos_embedding = nn.Embedding( num_patches, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([projection_dim])).to(DEVICE)\n",
        "        self.num_channels = num_channels\n",
        "    def forward(self, patch):\n",
        "       \n",
        "        unpack_shape = patch.shape\n",
        "        patch = torch.flatten(patch, start_dim = 2)\n",
        "        patch = torch.flatten(patch, end_dim = 1)\n",
        "        patch = self.projection(patch)\n",
        "        patch = nn.Unflatten(0, unpack_shape[:2])(patch)\n",
        "        \n",
        "        pos = torch.arange(0, self.num_patches).unsqueeze(0).repeat(patch.shape[0], 1).to(DEVICE)\n",
        "        \n",
        "        patch = self.dropout(patch/self.scale  + self.pos_embedding(pos))\n",
        "        \n",
        "        return patch\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caVP87bvBFfp"
      },
      "source": [
        "## ViT model\n",
        "\n",
        "Трасформерные блоки используют специальную версию selfattention из [статьи](https://arxiv.org/pdf/2110.11945.pdf). Взята с официального git repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Approx_GeLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.grad_checkpointing = True\n",
        "\n",
        "    def func(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.func(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def subtraction_gaussian_kernel_torch(q, k):\n",
        "    # [B, H, H1*W1, C] @ [C, H2*W2] -> [B, H, H1*W1, H2*W2]\n",
        "    matA_square = q ** 2. @ torch.ones(k.shape[-2:]).cuda()\n",
        "    # [H1*W1, C] @ [B, H, C, H2*W2] -> [B, H, H1*W1, H2*W2]\n",
        "    matB_square = torch.ones(q.shape[-2:]).cuda() @ k ** 2.\n",
        "    return matA_square + matB_square - 2. * (q @ k)\n",
        "\n",
        "\n",
        "class SoftmaxFreeAttentionKernel(nn.Module):\n",
        "    def __init__(self, dim, num_heads, q_len, k_len, num_landmark, use_conv, max_iter=20, kernel_method=\"torch\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_dim = int(dim // num_heads)\n",
        "        self.num_head = num_heads\n",
        "\n",
        "        self.num_landmarks = num_landmark\n",
        "        self.q_seq_len = q_len\n",
        "        self.k_seq_len = k_len\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "  \n",
        "        self.kernel_function = subtraction_gaussian_kernel_torch\n",
        "\n",
        "        ratio = int(np.sqrt(self.q_seq_len // self.num_landmarks))\n",
        "        if ratio == 1:\n",
        "            self.Qlandmark_op = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "            self.Qnorm_act = nn.Sequential(nn.LayerNorm(self.head_dim), nn.GELU())\n",
        "        else:\n",
        "            self.Qlandmark_op = nn.Conv2d(self.head_dim, self.head_dim, kernel_size=ratio, stride=ratio, bias=False)\n",
        "            self.Qnorm_act = nn.Sequential(nn.LayerNorm(self.head_dim), nn.GELU())\n",
        "\n",
        "        self.use_conv = use_conv\n",
        "        if self.use_conv:\n",
        "            self.conv = nn.Conv2d(\n",
        "                in_channels=self.num_head, out_channels=self.num_head,\n",
        "                kernel_size=(self.use_conv, self.use_conv), padding=(self.use_conv // 2, self.use_conv // 2),\n",
        "                bias=False,\n",
        "                groups=self.num_head)\n",
        "\n",
        "    def forward(self, Q, V):\n",
        "        b, nhead, N, headdim, = Q.size()\n",
        "        # Q: [b, num_head, N, head_dim]\n",
        "        Q = Q / math.sqrt(math.sqrt(self.head_dim))\n",
        "        K=Q\n",
        "        if self.num_landmarks == self.q_seq_len:\n",
        "            Q_landmarks = Q.reshape(b * self.num_head, int(np.sqrt(self.q_seq_len)) * int(np.sqrt(self.q_seq_len)) + 1,\n",
        "                                     self.head_dim)\n",
        "            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n",
        "            Q_landmarks = self.Qnorm_act(Q_landmarks).reshape(b, self.num_head, self.num_landmarks + 1, self.head_dim)\n",
        "            K_landmarks = Q_landmarks\n",
        "            attn = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n",
        "            attn = torch.exp(-attn / 2)\n",
        "            X = torch.matmul(attn, V)\n",
        "\n",
        "            h = w = int(np.sqrt(N))\n",
        "            if self.use_conv:\n",
        "                V_ = V[:, :, 1:, :]\n",
        "                cls_token = V[:, :, 0, :].unsqueeze(2)\n",
        "                V_ = V_.reshape(b, nhead, h, w, headdim)\n",
        "                V_ = V_.permute(0, 4, 1, 2, 3).reshape(b * headdim, nhead, h, w)\n",
        "                out = self.conv(V_).reshape(b, headdim, nhead, h, w).flatten(3).permute(0, 2, 3, 1)\n",
        "                out = torch.cat([cls_token, out], dim=2)\n",
        "                X += out\n",
        "        else:\n",
        "            Q_landmarks = Q.reshape(b * self.num_head, int(np.sqrt(self.q_seq_len)) * int(np.sqrt(self.q_seq_len)),\n",
        "                                    self.head_dim).reshape(b * self.num_head, int(np.sqrt(self.q_seq_len)),\n",
        "                                                           int(np.sqrt(self.q_seq_len)),\n",
        "                                                           self.head_dim).permute(0, 3, 1, 2)\n",
        "            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n",
        "            Q_landmarks = Q_landmarks.flatten(2).transpose(1, 2).reshape(b, self.num_head, self.num_landmarks,\n",
        "                                                                         self.head_dim)\n",
        "            Q_landmarks = self.Qnorm_act(Q_landmarks)\n",
        "            K_landmarks = Q_landmarks\n",
        "\n",
        "            kernel_1_ = self.kernel_function(Q, K_landmarks.transpose(-1, -2).contiguous())\n",
        "            kernel_1_ = torch.exp(-kernel_1_/2)\n",
        "\n",
        "            kernel_2_ = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n",
        "            kernel_2_ = torch.exp(-kernel_2_/2)\n",
        "\n",
        "            kernel_3_ = kernel_1_.transpose(-1, -2)\n",
        "\n",
        "            X = torch.matmul(torch.matmul(kernel_1_, self.newton_inv(kernel_2_)), torch.matmul(kernel_3_, V))\n",
        "\n",
        "            h = w = int(np.sqrt(N))\n",
        "            if self.use_conv:\n",
        "                V = V.reshape(b, nhead, h, w, headdim)\n",
        "                V = V.permute(0, 4, 1, 2, 3).reshape(b*headdim, nhead, h, w)\n",
        "                X += self.conv(V).reshape(b, headdim, nhead, h, w).flatten(3).permute(0, 2, 3, 1)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def newton_inv(self, mat):\n",
        "        P = mat\n",
        "        I = torch.eye(mat.size(-1), device=mat.device)\n",
        "        alpha = 2 / (torch.max(torch.sum(mat, dim=-1)) ** 2)\n",
        "        beta = 0.5\n",
        "        V = alpha * P\n",
        "        pnorm = torch.max(torch.sum(torch.abs(I - torch.matmul(P, V)), dim=-2))\n",
        "        err_cnt = 0\n",
        "        while pnorm > 1.01 and err_cnt < 10:\n",
        "            alpha *= beta\n",
        "            V = alpha * P\n",
        "            pnorm = torch.max(torch.sum(torch.abs(I - torch.matmul(P, V)), dim=-2))\n",
        "            err_cnt += 1\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            V = 2 * V - V @ P @ V\n",
        "        return V\n",
        "\n",
        "\n",
        "class SoftmaxFreeAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, q_len, k_len, num_landmark, conv_size, max_iter=20, kernel_method=\"cuda\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.grad_checkpointing = True\n",
        "        self.dim = dim\n",
        "        self.head_dim = int(dim // num_heads)\n",
        "        self.num_head = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(self.dim, self.num_head * self.head_dim)\n",
        "        self.W_v = nn.Linear(self.dim, self.num_head * self.head_dim)\n",
        "\n",
        "        self.attn = SoftmaxFreeAttentionKernel(dim, num_heads, q_len, k_len, num_landmark, conv_size, max_iter, kernel_method)\n",
        "\n",
        "        self.ff = nn.Linear(self.num_head * self.head_dim, self.dim)\n",
        "\n",
        "    def forward(self, X, return_QKV = False):\n",
        "\n",
        "        Q = self.split_heads(self.W_q(X))\n",
        "        V = self.split_heads(self.W_v(X))\n",
        "        attn_out = self.attn(Q, V)\n",
        "        attn_out = self.combine_heads(attn_out)\n",
        "\n",
        "        out = self.ff(attn_out)\n",
        "\n",
        "        if return_QKV:\n",
        "            return out, (Q, V)\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def combine_heads(self, X):\n",
        "        X = X.transpose(1, 2)\n",
        "        X = X.reshape(X.size(0), X.size(1), self.num_head * self.head_dim)\n",
        "        return X\n",
        "\n",
        "    def split_heads(self, X):\n",
        "        X = X.reshape(X.size(0), X.size(1), self.num_head, self.head_dim)\n",
        "        X = X.transpose(1, 2)\n",
        "        return X\n",
        "\n",
        "\n",
        "class SoftmaxFreeTransformer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, q_len, k_len, num_landmark, conv_size, drop_path=0., max_iter=20, kernel_method=\"torch\"):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = int(4*dim)\n",
        "\n",
        "        self.mha = SoftmaxFreeAttention(dim, num_heads, q_len, k_len, num_landmark, conv_size, max_iter, kernel_method)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(p=drop_path)\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "\n",
        "        self.ff1 = nn.Linear(self.dim, self.hidden_dim)\n",
        "        self.act = Approx_GeLU()\n",
        "        self.ff2 = nn.Linear(self.hidden_dim, self.dim)\n",
        "\n",
        "        self.dropout2 = torch.nn.Dropout(p=drop_path)\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "\n",
        "    def forward(self, X, return_QKV = False):\n",
        "\n",
        "        if return_QKV:\n",
        "            mha_out, QKV = self.mha(X, return_QKV = True)\n",
        "        else:\n",
        "            mha_out = self.mha(X)\n",
        "\n",
        "        mha_out = self.norm1(X + self.dropout1(mha_out))\n",
        "        ff_out = self.ff2(self.act(self.ff1(mha_out)))\n",
        "        mha_out = self.norm2(mha_out + self.dropout2(ff_out))\n",
        "\n",
        "        if return_QKV:\n",
        "            return mha_out, QKV\n",
        "        else:\n",
        "            return mha_out\n",
        "\n",
        "\n",
        "class SoftmaxFreeTrasnformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, H, W, drop_path=0., conv_size=3, max_iter=20, kernel_method=\"torch\"):\n",
        "        super().__init__()\n",
        "        seq_len = 64\n",
        "        self.att = SoftmaxFreeTransformer(dim, num_heads, int(H*W), int(H*W), seq_len, conv_size, drop_path, max_iter, kernel_method)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.att(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YxdMF_8u3EJ3"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Layer(nn.Module):\n",
        "    def __init__(self, num_heads, hid_dim,mlp_layers_dims, num_patches, dropout = 0.1):\n",
        "        super(Transformer_Encoder_Layer, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.norm_before_multihead = nn.LayerNorm(hid_dim)\n",
        "        self.multihead =   SoftmaxFreeTrasnformerBlock(hid_dim, num_heads,int(np.sqrt(num_patches)), int(np.sqrt(num_patches)))\n",
        "        self.norm_after_multihead = nn.LayerNorm(hid_dim)\n",
        "        self.mlp = MLP(mlp_layers_dims).to(DEVICE)\n",
        "        self.mlp.apply(init_weights)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    def forward(self, x):\n",
        "        # Layer normalization 1.\n",
        "        _x = x.clone()\n",
        "        x = self.norm_before_multihead(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output= self.multihead(x)\n",
        "        # Skip connection 1.\n",
        "        x = _x + attention_output\n",
        "        # Layer normalization 2.\n",
        "        _x = x.clone()\n",
        "        x = self.norm_after_multihead(x)\n",
        "        # MLP.\n",
        "        x = self.mlp.forward(x)\n",
        "        # Skip connection 2.\n",
        "        x  =_x + x\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qYlQY0pcM9KJ"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPFzIyks7TYk"
      },
      "source": [
        "class VIT(nn.Module):\n",
        "    def __init__(self, patch_size,num_patches,projection_dim,transformer_layers_config,mlp_head_layers_dim , num_channels = 3, start_head = False):\n",
        "        super(VIT, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "        #дополнительная начальная CNN\n",
        "        self.start_head = start_head\n",
        "        if(self.start_head == True):\n",
        "            \n",
        "            self.start_CNN =  CNN().to(DEVICE)\n",
        "        self.patches = Patches(patch_size, num_channels = num_channels)\n",
        "        self.patch_encoding = PatchEncoder(num_patches, patch_size,projection_dim, num_channels = num_channels)\n",
        "        self.transformer_layers = []\n",
        "        for num_heads, config in transformer_layers_config:\n",
        "            self.transformer_layers.append (Transformer_Encoder_Layer(num_heads, projection_dim,config,num_patches ).to(DEVICE))\n",
        "        self.final_representation= nn.Sequential(\n",
        "            nn.LayerNorm( transformer_layers_config[-1][1][-1][1]),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.MLP_head = MLP(mlp_head_layers_dim ).to(DEVICE)\n",
        "        self.MLP_head.apply(init_weights)\n",
        "\n",
        "    def forward(self,x):\n",
        "      if(self.start_head == True):\n",
        "        x = self.start_CNN(x)\n",
        "\n",
        "      x = self.patches(x)\n",
        "      x = self.patch_encoding(x)\n",
        "      for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "      x = self.final_representation(x)\n",
        "      x = self.MLP_head(x)\n",
        "      #log soft max устойчивей чем простой\n",
        "      output = F.log_softmax(x, dim = 1)\n",
        "      return output\n"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3HnixhyBFfr"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF3WMwn5_1wA"
      },
      "source": [
        "def train_single_epoch(model: torch.nn.Module,\n",
        "                       optimizer: torch.optim.Optimizer, \n",
        "                       loss_function: torch.nn.Module, \n",
        "                       train_loader):\n",
        "    \n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    accuracy = 0\n",
        "    n =0 \n",
        "\n",
        "    for x,y in tqdm(train_loader):\n",
        "        x = x.to(DEVICE)\n",
        "        n += x.shape[0]\n",
        "        y = y.to(DEVICE)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        hyp = model(x)\n",
        "        y_pred = hyp.argmax(dim = 1, keepdim = True).to(DEVICE)\n",
        "        accuracy += y_pred.eq(y.view_as(y_pred)).sum().item()\n",
        "        \n",
        "       \n",
        "        loss = loss_function(hyp, y)\n",
        "        loss.backward()\n",
        "        loss_sum += loss\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "\n",
        "    return {'loss' : loss_sum.item()/n,'accuracy' : accuracy/n}\n",
        "    \n",
        "    "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHsgHtzXA7Eh"
      },
      "source": [
        "@torch.no_grad()\n",
        "def validate_single_epoch(model: torch.nn.Module,\n",
        "                          loss_function: torch.nn.Module,                          \n",
        "                          test_loader):\n",
        "    model.eval()\n",
        "    loss_sum = 0\n",
        "    accuracy = 0\n",
        "    n = 0\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        n += x.shape[0]\n",
        "        y = y.to(DEVICE)\n",
        "     \n",
        "        \n",
        "\n",
        "        hyp = model(x)\n",
        "        y_pred = hyp.argmax(dim = 1, keepdim = True).to(DEVICE)\n",
        "        accuracy += y_pred.eq(y.view_as(y_pred)).sum().item()\n",
        "        loss = loss_function(hyp, y)\n",
        "        loss_sum += loss\n",
        "\n",
        "      \n",
        "    loss_avr = loss_sum /n\n",
        "\n",
        "    \n",
        "    return {'loss' : loss_avr.item(),'accuracy' : accuracy/n}"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QSWsAksCABK"
      },
      "source": [
        "def train_model(model: torch.nn.Module, \n",
        "                train_data,\n",
        "              \n",
        "                test_data,\n",
        "                \n",
        "                augmentation,\n",
        "                transformation,\n",
        "\n",
        "                save_link,\n",
        "                loss_function: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
        "                optimizer_class: Type[torch.optim.Optimizer] = torch.optim.AdamW,\n",
        "                optimizer_params: Dict = { 'betas':  (0.9, 0.999), 'eps' :1e-9 , 'weight_decay' : 1e-4  },\n",
        "                initial_lr = 0.01,\n",
        "                lr_scheduler_class: Any = torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "                lr_scheduler_params: Dict = {},\n",
        "                batch_size = 16,\n",
        "                max_epochs = 1000,\n",
        "                early_stopping_patience = 20):\n",
        "    # set to training mode\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # Everything is ready for the training\n",
        "  \n",
        "    model.to(DEVICE)\n",
        "    optimizer = optimizer_class(model.parameters(), lr=initial_lr, **optimizer_params)\n",
        "    lr_scheduler = lr_scheduler_class(optimizer, **lr_scheduler_params)\n",
        "    \n",
        "    best_val_loss = None\n",
        "    best_epoch = None\n",
        "    loss_list = {'train': list(), 'valid': list()}\n",
        "    \n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        \n",
        "        print(f'Epoch {epoch}')\n",
        "    \n",
        "        train_loss =  train_single_epoch(model, optimizer, loss_function,train_loader)\n",
        "        print(f'Train metrics: \\n{train_loss}')\n",
        "        print('Validating epoch\\n')\n",
        "\n",
        "        val_metrics = validate_single_epoch(model, loss_function,test_loader)\n",
        "        loss_list['valid'].append(val_metrics['loss'])\n",
        "        loss_list['train'].append(train_loss['loss'])\n",
        "        print(f'Validation metrics: \\n{val_metrics}')\n",
        "\n",
        "        lr_scheduler.step(val_metrics['loss'])\n",
        "        \n",
        "        if best_val_loss is None or best_val_loss > val_metrics['loss']:\n",
        "            print(f'Best model yet, saving')\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            best_epoch = epoch\n",
        "         \n",
        "           \n",
        "            torch.save(model,save_link)\n",
        "            \n",
        "        if epoch - best_epoch > early_stopping_patience:\n",
        "            print('Early stopping triggered')\n",
        "            ploting_curves(loss_list,best_epoch)\n",
        "            break\n",
        "    return model, loss_list \n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08L8KBBj8Gdr"
      },
      "source": [
        "##Model config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mA5_VlWqGQ9"
      },
      "source": [
        "Здесь происходит обуение и настройка параметров, можете экспериментировать и пробовать разные штуки на дефолтной модели нейронки, крайне советую почитать статью и посмотреть на их параметры для обучения.\n",
        "\n",
        "Можно менять такие вещи как: оптимизатор, расписание и значение learning rate, финальную MLP голову модели и количество голов в трансформере, размерность скрытого измерения. Так же можно изначально поставить CNN сеть и из её выходов делать патчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-oPkSrxBFfh"
      },
      "source": [
        "\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "num_channels = 3\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_classes = 10"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz27zs9WJfwd"
      },
      "source": [
        "Настройка стартовой головы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9I_TKyQkzZ6"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.layer = nn.Sequential(nn.Conv2d(3, 4, 2), \n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(4,8, 2,1),\n",
        "\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=1)) \n",
        "        self.layer.apply(init_weights)\n",
        "    def forward(self, x):\n",
        "            return self.layer(x)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl_4cf1I4kGZ"
      },
      "source": [
        "#здесь задаётся MLP в кадом слое трансформера\n",
        "#один слой задётся tuple (кол-во голов в multiheadattention(должно быть делителем projection_dim) , list из слоёв в mlp первый и последний должны быть размерности projection_dim)\n",
        "transformer_layers_config = [\n",
        "                             (2,  [(projection_dim, 64), (64 ,128 ),(128 ,256 ), (256, projection_dim)])\n",
        "                             \n",
        "\n",
        "                         ]\n",
        "#тут задаётся только главная голова MLP\n",
        "#Если сиспользуйте стартовую голову CNN не забудье поменять num_patches на new_num_patches\n",
        "mlp_head_layers_dim = [(projection_dim*num_patches,256), (256,512) , ( 512, num_classes)]\n"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnZBtdCLe2Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bda181-ea10-4d51-e3fd-9bd7e4c86374"
      },
      "source": [
        "#если используйте стартовую CNN голову не забудьте поменять: patch_size , num_patches, num_channels на new_...\n",
        "model = VIT(patch_size = patch_size, \n",
        "            num_patches = num_patches,\n",
        "            projection_dim = projection_dim,\n",
        "            transformer_layers_config = transformer_layers_config,\n",
        "            mlp_head_layers_dim = mlp_head_layers_dim,\n",
        "            num_channels=num_channels,\n",
        "            start_head= False)\n",
        "model.to(DEVICE)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VIT(\n",
              "  (patches): Patches()\n",
              "  (patch_encoding): PatchEncoder(\n",
              "    (projection): Linear(in_features=768, out_features=64, bias=True)\n",
              "    (pos_embedding): Embedding(196, 64)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (final_representation): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (MLP_head): MLP()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY2BbIT34xni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397a4045-80e7-451a-e2a2-39ce005d2f3a"
      },
      "source": [
        "print('Total number of trainable parameters', \n",
        "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters 61888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHEQ3qZRByvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d93202-aa91-418e-ebb0-9fc7055ddc51"
      },
      "source": [
        "model.apply(init_weights)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VIT(\n",
              "  (patches): Patches()\n",
              "  (patch_encoding): PatchEncoder(\n",
              "    (projection): Linear(in_features=768, out_features=64, bias=True)\n",
              "    (pos_embedding): Embedding(196, 64)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (final_representation): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (MLP_head): MLP()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkGGW_FS8Kzx"
      },
      "source": [
        "##Configure hyperparametrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVc79-SuNQ6"
      },
      "source": [
        "[Трансформации](https://pytorch.org/vision/stable/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnBFtaTBFfl"
      },
      "source": [
        "#можете вертеть, отзеркаливать, вырезать части и блюрить картинки. Следите за тем, чтобы ответ при этом не менялся (например отзеркалить цифру 2 нельзя, это уже будет 5)\n",
        "transformation  = torchvision.transforms.Compose([\n",
        "                                      \n",
        "])\n",
        "#тоже самое, но это рфботает нестабильно, не используйте                                                ])\n",
        "augmentation  =  iaa.Sequential([\n",
        "                                  iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
        "                                   iaa.GaussianBlur(sigma=(0, 3.0)),\n",
        "                                   iaa.Flipud(0.5),\n",
        "                                   iaa.Affine(rotate = (-30,30),scale=(0.5, 1.5))                                                    \n",
        "                                  ])\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z97hp00buRif"
      },
      "source": [
        "[learning rate](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg7bHLIS5YNk"
      },
      "source": [
        "#Это расписание learning rate с постепенным линейным нагревом, а потом убываением. Говорят, без него трансформер просто не обучить, так что обучайте на этом\n",
        "initial_lr  = 5e-2\n",
        "warmup_epoch = 100\n",
        "def lambda_lr (epoch):\n",
        "    if(epoch <= warmup_epoch): \n",
        "        return initial_lr*(epoch+1) \n",
        "    else: \n",
        "        return   initial_lr/np.sqrt(epoch+1)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  =torch.load('/content/drive/MyDrive/best_VIT_soft_model_2.pth')"
      ],
      "metadata": {
        "id": "hhhACpSMT9KM"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZTTtYSwVjpD",
        "outputId": "8a478f70-3d34-49df-dc51-072a74ee5018"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VIT(\n",
              "  (patches): Patches()\n",
              "  (patch_encoding): PatchEncoder(\n",
              "    (projection): Linear(in_features=768, out_features=64, bias=True)\n",
              "    (pos_embedding): Embedding(196, 64)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (final_representation): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (MLP_head): MLP()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0cbryD65EZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50394c94-9920-4f6a-b10e-2a9cc53113ed"
      },
      "source": [
        "best_model,loss_list = train_model(model, #cама модель \n",
        "                trainset,\n",
        "                testset,\n",
        "                #Сюда забейте ссылку куда кидать модели, обязательно их сохраняйте!!!!\n",
        "\n",
        "                save_link =  '/content/drive/MyDrive/best_VIT_soft_model.pth',\n",
        "                #преобразование данных\n",
        "                augmentation = None,\n",
        "                transformation = transformation,\n",
        "                #оптимизатор\n",
        "                optimizer_class = torch.optim.AdamW,\n",
        "                optimizer_params =  { 'betas':  (0.9, 0.999), 'eps' :1e-9 , 'weight_decay' : 1e-4  },\n",
        "                #расписание learning rate\n",
        "                lr_scheduler_class = torch.optim.lr_scheduler.MultiplicativeLR,\n",
        "                lr_scheduler_params  = {'lr_lambda' : lambda_lr},\n",
        "                \n",
        "                batch_size = 16,\n",
        "                initial_lr = initial_lr )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 303/3125 [00:37<05:48,  8.09it/s]"
          ]
        }
      ]
    }
  ]
}